{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "765c0422",
   "metadata": {},
   "source": [
    "# 문제 6\n",
    "\n",
    "[Kaggle 형] train_prob.csv로 문제 failure 예측하는 모델을 만들고, \n",
    "\n",
    "test_prob.csv에 대한 failure가 1일 확률 예측하여 다음과 같은 형식의 answer6.csv를 만들어라. \n",
    "\n",
    "측정 지표는 AUC(area under of ROC curve)이다. id 는 테스트 케이스의 id 이고, failure에는 failure가 1이 될 확률이다.\n",
    "\n",
    "id,failure\n",
    "\n",
    "16115, 0.1\n",
    "\n",
    "16116, 0.2\n",
    "\n",
    "\n",
    "**강사: 멀티캠퍼스 강선구(sunku0316.kang@multicampus.com, sun9sun9@gmail.com)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "526638d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 20:34:20) [MSC v.1916 64 bit (AMD64)]\n",
      "pandas 0.25.1\n",
      "numpy 1.18.5\n",
      "sklearn 0.21.3\n",
      "scipy 1.5.2\n",
      "mlxtend 0.15.0.0\n",
      "statsmodels 0.11.1\n",
      "xgboost 0.80\n"
     ]
    }
   ],
   "source": [
    "# 실행 환경 확인\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import scipy\n",
    "import statsmodels\n",
    "import mlxtend\n",
    "import sys\n",
    "import xgboost as xgb\n",
    "\n",
    "print(sys.version)\n",
    "for i in [pd, np, sklearn, scipy, mlxtend, statsmodels, xgb]:\n",
    "    print(i.__name__, i.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "42ff1fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train_prob.csv', index_col='id')\n",
    "df_test = pd.read_csv('test_prob.csv', index_col='id')\n",
    "df_ans = pd.read_csv('test_prob_ans.csv', index_col='id') # 실제 시험에서는 존재 x, 자가 채점용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cc284311",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[['na_1', 'na_2']] = df_train[['measurement_3', 'measurement_5']].isna()\n",
    "df_test[['na_1', 'na_2']] = df_test[['measurement_3', 'measurement_5']].isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "df2a6f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D    5112\n",
       "Name: product_code, dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['product_code'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "01a13835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C    5765\n",
       "E    5343\n",
       "B    5250\n",
       "A    5100\n",
       "Name: product_code, dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['product_code'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "51c08527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 단계에서 Iterative Imputer를 통한 결측치를 처리하는 루틴을 가져옵니다.\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "imp = IterativeImputer(\n",
    "    estimator = LinearRegression(fit_intercept=True),\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "X_imp = ['measurement_{}'.format(i) for i in range(3, 10)] + ['measurement_17']\n",
    "\n",
    "df_train[X_imp] = df_train.groupby('product_code')[X_imp].apply(\n",
    "    lambda x: pd.DataFrame(imp.fit_transform(x[X_imp]), index=x.index, columns=x.columns)\n",
    ")\n",
    "df_test[X_imp] = df_test.groupby('product_code')[X_imp].apply(\n",
    "    lambda x: pd.DataFrame(imp.fit_transform(x[X_imp]), index=x.index, columns=x.columns)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6490aadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법 1로 구현합니다. (참고)\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer# 구문을 사용하여 실험 단계인 모듈을 활성화하고, \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X_imp = ['measurement_{}'.format(i) for i in range(3, 10)] + ['measurement_17']\n",
    "# train에 등장하지 않은 수준이 있습니다, test를 포함하여 결측처리 모델을 만듭니다.\n",
    "s_imp = pd.concat([\n",
    "        df_train[X_imp + ['product_code']],\n",
    "        df_test[X_imp + ['product_code']]\n",
    "], axis=0).groupby('product_code')\\\n",
    ".apply(\n",
    "    lambda x: IterativeImputer(estimator=LinearRegression(),random_state=123).fit(x[X_imp])\n",
    ")\n",
    "# train에 적용합니다.\n",
    "df_train[X_imp] = df_train[X_imp + ['product_code']]\\\n",
    "            .groupby('product_code')\\\n",
    "            .apply(\n",
    "                lambda x: pd.DataFrame(s_imp.loc[x.name].transform(x[X_imp]), index=x.index, columns=X_imp)\n",
    "            )\n",
    "# test에 적용합니다.\n",
    "df_test[X_imp] = df_test[X_imp + ['product_code']]\\\n",
    "            .groupby('product_code')\\\n",
    "            .apply(\n",
    "                lambda x: pd.DataFrame(s_imp.loc[x.name].transform(x[X_imp]), index=x.index, columns=X_imp)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e85975b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean = ['measurement_{}'.format(i) for i in range(10, 17)]\n",
    "df_train[X_mean] = df_train.groupby('product_code')[X_mean].transform(lambda x: x.fillna(x.mean()))\n",
    "df_test[X_mean] = df_test.groupby('product_code')[X_mean].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ed63edb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 합쳐서 구해봅니다. (참고)\n",
    "X_mean = ['measurement_{}'.format(i) for i in range(10, 17)]\n",
    "# 역시 train에 등장하지 않은 수준을 처리하기 위해 합치니다.\n",
    "df_mean = pd.concat([\n",
    "            df_train[['product_code'] + X_mean],\n",
    "            df_test[['product_code'] + X_mean]\n",
    "        ]).groupby('product_code')[X_mean].agg('mean')\n",
    "\n",
    "df_train[X_mean] = df_train.groupby('product_code')[X_mean]\\\n",
    "            .apply(lambda x: pd.DataFrame(x.fillna(df_mean.loc[x.name]), index=x.index, columns=x.columns))\n",
    "df_test[X_mean] = df_test.groupby('product_code')[X_mean]\\\n",
    "            .apply(lambda x: pd.DataFrame(x.fillna(df_mean.loc[x.name]), index=x.index, columns=x.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ae4a9e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['loading'] = df_train['loading'].fillna(df_train['loading'].mean())\n",
    "df_test['loading'] = df_test['loading'].fillna(df_train['loading'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75292c6",
   "metadata": {},
   "source": [
    "# Kaggle형 풀이 단계\n",
    "\n",
    "Step 1: 검증 방법을 정하고, 검증 루틴을 만듭니다.\n",
    "\n",
    "Step 2: Baseline 모델을 만듭니다\n",
    "\n",
    "Step 3: 모델 선택 루틴을 만듭니다.\n",
    "\n",
    "Step 4: 모델 개선 작업을 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "c8ce0d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_hist = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "f2bb83e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A' 'B' 'E'] ['C']\n",
      "['A' 'B' 'C'] ['E']\n",
      "['A' 'C' 'E'] ['B']\n",
      "['B' 'C' 'E'] ['A']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "X_all = df_test.columns.tolist()\n",
    "\n",
    "# Step1: 검증 방법을 정합니다. 4-fold Grouped CV \n",
    "gcv = GroupKFold(4)\n",
    "\n",
    "# GroupKFold의 특징을 보여드리기 위한 루틴입니다.\n",
    "# 실제로는 필요한 코드는 아니지만 참고용으로 남겨 둡니다.\n",
    "# Validation set에는 Validation Train에 등장하지 않은 범주값으로 구성이 되도록하는 합니다.\n",
    "for train_idx, test_idx in gcv.split(df_train[X_all], df_train['failure'], groups=df_train['product_code']):\n",
    "    print(df_train.iloc[train_idx]['product_code'].unique(), df_train.iloc[test_idx]['product_code'].unique())\n",
    "\n",
    "\n",
    "def eval_model(model):\n",
    "    return cross_validate(\n",
    "        model, df_train[X_all], df_train['failure'], \n",
    "        groups=df_train['product_code'], scoring='roc_auc', cv=gcv, return_train_score=True\n",
    "    )\n",
    "\n",
    "def print_result(model_name, result):\n",
    "    output = 'Valid: {:.5f}±{:.5f},  V.Train: {:.5f}±{:.5f}'.format( \n",
    "            np.mean(result['test_score']), np.std(result['test_score']),\n",
    "            np.mean(result['train_score']), np.std(result['train_score']),\n",
    "        )\n",
    "    print(output)\n",
    "    s_hist.append(pd.Series([model_name, output], index=['model name', 'result']))\n",
    "\n",
    "# Step 3: 모델 선택 루틴입니다.\n",
    "def select_model(model):\n",
    "    # 전체 데이터셋으로 학습을 합니다.\n",
    "    model.fit(df_train[X_all], df_train['failure'])\n",
    "    # 평가 데이터셋으로 예측합니다.\n",
    "    prd = model.predict_proba(df_test[X_all])[:, 1]\n",
    "    # 제출 파일을 만듭니다.\n",
    "    pd.DataFrame({\n",
    "        'id': df_test.index,\n",
    "        'failure': prd\n",
    "    }).to_csv('answer6.csv', index=None)\n",
    "    return prd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "0d6b81e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: 0.58937±0.00379,  V.Train: 0.59191±0.00146\n"
     ]
    }
   ],
   "source": [
    "# Step2 Baseline 모델을 만들어 검증합니다.\n",
    "# Baseline: LR - SFS - ['loading', 'measurement_1', 'measurement_4', 'measurement_14', 'measurement_17', 'na_1']\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_lr = make_pipeline(\n",
    "    ColumnTransformer([\n",
    "        ('std', StandardScaler(), ['loading', 'measurement_1', 'measurement_4', 'measurement_14', 'measurement_17']),\n",
    "        ('pt', 'passthrough', ['na_1'])\n",
    "    ]), \n",
    "    LogisticRegression(solver='lbfgs')\n",
    ")\n",
    "result = eval_model(clf_lr)\n",
    "print_result('baseline', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "16793545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline 채점 결과: 0.5883988309352517\n"
     ]
    }
   ],
   "source": [
    "prd = select_model(clf_lr)\n",
    "print(\"Baseline 채점 결과:\", roc_auc_score(df_ans['failure'], prd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "ca046a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: 0.58857±0.00294,  V.Train: 0.59163±0.00102\n"
     ]
    }
   ],
   "source": [
    "# lr2: LR + feature PCA(n_components = 7)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "    ('std_pca', make_pipeline(StandardScaler(), PCA(n_components=7)), ['measurement_{}'.format(i) for i in range(18)]),\n",
    "    ('std', StandardScaler(), ['loading']),\n",
    "    ('pt', 'passthrough', ['na_1'])\n",
    "])\n",
    "clf_lr2 = make_pipeline(ct, LogisticRegression(solver='lbfgs'))\n",
    "result = eval_model(clf_lr2)\n",
    "print_result('lr2', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "10d25bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: 0.58935±0.00389,  V.Train: 0.59181±0.00145\n"
     ]
    }
   ],
   "source": [
    "# lr3: Basline + np.log (loading)\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "    ('std', StandardScaler(), ['measurement_1', 'measurement_4', 'measurement_14', 'measurement_17']),\n",
    "    ('log_std', make_pipeline(FunctionTransformer(func=np.log, validate=False), StandardScaler()), ['loading']),\n",
    "    ('pt', 'passthrough', ['na_1'])\n",
    "])\n",
    "clf_lr3 = make_pipeline(ct, LogisticRegression(solver='lbfgs'))\n",
    "result = eval_model(clf_lr3)\n",
    "print_result('lr3', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "e7ce2978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: 0.58716±0.00329,  V.Train: 0.59336±0.00160\n"
     ]
    }
   ],
   "source": [
    "# LDA\n",
    "# TODO: 속성선택도 적용해봅니다.\n",
    "clf_lda = make_pipeline(\n",
    "    ColumnTransformer([\n",
    "        ('std', StandardScaler(), ['measurement_{}'.format(i) for i in range(18)] + ['loading']),\n",
    "        #('std', StandardScaler(), ['loading', 'measurement_1', 'measurement_4', 'measurement_14', 'measurement_17']),\n",
    "        ('pt', 'passthrough', ['na_1'])\n",
    "    ]), \n",
    "    LinearDiscriminantAnalysis()\n",
    ")\n",
    "result = eval_model(clf_lda)\n",
    "print_result('lda', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "60f55e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA 채점 결과: 0.5894689748201439\n"
     ]
    }
   ],
   "source": [
    "prd = select_model(clf_lda)\n",
    "print(\"LDA 채점 결과:\", roc_auc_score(df_ans['failure'], prd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "0e558339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.ensemble import RandomForestClassifier\\nct = ColumnTransformer([\\n    ('pt', 'passthrough', ['loading', 'na_1', 'na_2'] + ['measurement_{}'.format(i) for i in range(18)])\\n])\\nclf_rf = make_pipeline(ct, RandomForestClassifier(\\n    n_estimators=?, max_depth=?, min_samples_split=?, random_state=123, \\n))\\nresult = eval_model(clf_rf)\\nprint_result('rf', result)\\n\""
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RF: RandomForestClassifier: {'max_depth': 7, 'min_samples_split': 512, 'n_estimators': 15} 튜닝\n",
    "# TODO: 수업시간에 튜닝을 해봅니다.\n",
    "\"\"\"\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "ct = ColumnTransformer([\n",
    "    ('pt', 'passthrough', ['loading', 'na_1', 'na_2'] + ['measurement_{}'.format(i) for i in range(18)])\n",
    "])\n",
    "clf_rf = make_pipeline(ct, RandomForestClassifier(\n",
    "    n_estimators=?, max_depth=?, min_samples_split=?, random_state=123, \n",
    "))\n",
    "result = eval_model(clf_rf)\n",
    "print_result('rf', result)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "38edbf1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\\nfrom sklearn.ensemble import RandomForestClassifier\\n\\nct = ColumnTransformer([\\n    ('std_lda', ? , ['measurement_{}'.format(i) for i in range(18)]),\\n    ('pt', 'passthrough', ['loading', 'na_1', 'na_2'])# + ['measurement_{}'.format(i) for i in range(18)])\\n])\\nclf_rf2 = make_pipeline(ct, RandomForestClassifier(\\n    n_estimators=?, max_depth=?, min_samples_split=?, random_state=123,\\n))\\nresult = eval_model(clf_rf2)\\nprint_result('rf2', result)\\n\""
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RF: RandomForestClassifier + LinearDiscriminantAnalysis\n",
    "# TODO: LDA를 결합시키는 루틴을 만들어 보겠습니다.\n",
    "\"\"\"\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "    ('std_lda', ? , ['measurement_{}'.format(i) for i in range(18)]),\n",
    "    ('pt', 'passthrough', ['loading', 'na_1', 'na_2'])# + ['measurement_{}'.format(i) for i in range(18)])\n",
    "])\n",
    "clf_rf2 = make_pipeline(ct, RandomForestClassifier(\n",
    "    n_estimators=?, max_depth=?, min_samples_split=?, random_state=123,\n",
    "))\n",
    "result = eval_model(clf_rf2)\n",
    "print_result('rf2', result)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "52deaff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport xgboost as xgb\\nct = ColumnTransformer([\\n    ('pt', 'passthrough', ['loading', 'na_1', 'na_2'] + ['measurement_{}'.format(i) for i in range(18)])\\n])\\nclf_xgb = xgb.XGBClassifier(\\n    learning_rate=?, n_estimators=?, subsample=?, colsample_bytree=?, max_depth=?, random_state=123\\n)\\nclf_xgb = make_pipeline(ct, clf_xgb)\\nresult = eval_model(clf_xgb)\\nprint_result('xgb', result)\\n\""
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGB: XGBoost\n",
    "# TODO: 튜닝을 같이 해봅니다.\n",
    "\"\"\"\n",
    "import xgboost as xgb\n",
    "ct = ColumnTransformer([\n",
    "    ('pt', 'passthrough', ['loading', 'na_1', 'na_2'] + ['measurement_{}'.format(i) for i in range(18)])\n",
    "])\n",
    "clf_xgb = xgb.XGBClassifier(\n",
    "    learning_rate=?, n_estimators=?, subsample=?, colsample_bytree=?, max_depth=?, random_state=123\n",
    ")\n",
    "clf_xgb = make_pipeline(ct, clf_xgb)\n",
    "result = eval_model(clf_xgb)\n",
    "print_result('xgb', result)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "470bed12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>baseline</td>\n",
       "      <td>Valid: 0.58937±0.00379,  V.Train: 0.59191±0.00146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>lda</td>\n",
       "      <td>Valid: 0.58716±0.00329,  V.Train: 0.59336±0.00160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>lr2</td>\n",
       "      <td>Valid: 0.58857±0.00294,  V.Train: 0.59163±0.00102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>lr3</td>\n",
       "      <td>Valid: 0.58935±0.00389,  V.Train: 0.59181±0.00145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       result\n",
       "model name                                                   \n",
       "baseline    Valid: 0.58937±0.00379,  V.Train: 0.59191±0.00146\n",
       "lda         Valid: 0.58716±0.00329,  V.Train: 0.59336±0.00160\n",
       "lr2         Valid: 0.58857±0.00294,  V.Train: 0.59163±0.00102\n",
       "lr3         Valid: 0.58935±0.00389,  V.Train: 0.59181±0.00145"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(s_hist).groupby('model name').last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "ded79d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nclf_vt = VotingClassifier([\\n    ('baseline', clf_lr), # LR + SFS\\n    ('lr2', clf_lr2), # LR.2: LR + feature PCA\\n    ('lr3', clf_lr3), # Basline + np.log (loading)\\n    ('lda', clf_lda), # LDA\\n    ('rf', clf_rf), # RF 튜닝\\n    ('rf2', clf_rf2), # RF + LDA\\n    ('xgb', clf_xgb), # XGB\\n], voting='soft')\\nresult = eval_model(clf_vt)\\nresult,  np.mean(result['test_score']), np.mean(result['train_score'])\\n\""
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Voting을 합니다.\n",
    "# TODO: 같이 Voting 모델까지 만들어봅니다.\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\"\"\"\n",
    "clf_vt = VotingClassifier([\n",
    "    ('baseline', clf_lr), # LR + SFS\n",
    "    ('lr2', clf_lr2), # LR.2: LR + feature PCA\n",
    "    ('lr3', clf_lr3), # Basline + np.log (loading)\n",
    "    ('lda', clf_lda), # LDA\n",
    "    ('rf', clf_rf), # RF 튜닝\n",
    "    ('rf2', clf_rf2), # RF + LDA\n",
    "    ('xgb', clf_xgb), # XGB\n",
    "], voting='soft')\n",
    "result = eval_model(clf_vt)\n",
    "result,  np.mean(result['test_score']), np.mean(result['train_score'])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e143e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
